# 如何知道自己的 IP Address？

### 從 `ifconfig` 找 Local IP

這個指令會列出所有與 network 相關的資訊，需要人工找到 `en0` 或 `en1` 區段（`status: active` 的那個），這個區段的其中有一行會是 `inet` 開頭，緊接在 `inet` 後的就是你的 local IP。

Example Output:

```plaintext
en0: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
    options=400<CHANNEL_IO>
    ether 0c:e4:41:e5:cc:8f 
    inet6 fe80::866:7f6e:6f5d:4728%en0 prefixlen 64 secured scopeid 0xb 
    inet 192.168.50.88 netmask 0xffffff00 broadcast 192.168.50.255
    nd6 options=201<PERFORMNUD,DAD>
    media: autoselect
    status: active
```

### `ipconfig getifaddr en0` 直接得到 Local IP

請注意，是 `ipconfig`，不是 `ifconfig`，這個指令會直接 output 一行 local IP，如果輸入 `en0` 發現沒有 output 就換 `en1`。

### `curl ifconfig.me` 取得 Public IP

如果你所連到的網路在 [[NAT]] 底下，那就必須從外部才能知道自己的 public IP，因此這裡要使用 `curl` 發 request 到 `https://ifconfig.me` 這個外部網站，取得自己的 public IP 後 stdout。

其實並不是只有 `https://ifconfig.me` 可以查詢 Public IP，其他網站（如 `https://ipinfo.io`）甚至提供了更完整的資訊，所以也可以試試看 `curl ipinfo.io`。

# `ping <HOST>`

`<HOST>` 的部分可以填入 IP 或 domain，但**不可**加上 protocol, url path, port, query string 等額外資訊。

`ping` 使用 ICMP (Internet Control Message Protocol) 來確認目標 IP/domain 是否可以在網路上被找到，因此 ==ping host 成功只能代表網路的 routing 沒問題，並不代表該 host 可以正常提供服務==。

e.g.

```bash
ping www.google.com
```

# `curl <URL>`

`curl` 可以用來傳送資料給 server 以及接收從 server 來的資料，其支援包含 FILE, FTP, FTPS, HTTP, HTTPS, WS, WSS… 等幾乎所有你想得到的 protocols，是一個非常全面的指令。

`<URL>` 前面若沒加 protocol，`curl` 就會根據 `<URL>` 猜測最有可能的 protocol（通常是`http`），然後幫你加上去，例如：

```bash
curl www.google.com  # will become "http://www.google.com"
```

### 依規則連續發送 Requests

可以使用 `{}` 以及 `[]` 來連續發送多個 requests，其中使用 `{}` 時，裡面要用 `,` 分隔多個想 iterate 的 strings：

```bash
curl "http://site.{one,two,three}.com"
```

使用 `[]` 則是搭配 `-` 指定一個區間、搭配 `:` 指定間隔，間隔可以不寫（預設是 1）：

```bash
curl "http://example.com/file[1-10].txt"
# or
curl "http://example.com/file[a-z:2].txt"
```

請記得，使用這些花式的 URL 時，`<URL>` 的外面要用 `""` 包住。

### 下載檔案（將 Response 寫入 Local 的 Disk）

使用 `-o <LOCAL_FILE_NAME>` 或 `--output <LOCAL_FILE_NAME>` option，即可指定將 response 寫入 local 的檔案並命名為 `<LOCAL_FILE_NAME>`：

```bash
curl -o my_file https://example.com
```

如果搭配連發 requests 的操作，則可以在 `<LOCAL_FILE_NAME>` 中用變數 `#1`, `#2`, … 來代表第一區塊、第二區塊… 現在 iterate 到的 string：

```bash
curl "http://{site,host}.host[1-5].com" -o "#1_#2"
```

這樣一來 file name 就會是 site_1, site_2, …, host_1, host_2, …。

如果不想特別想檔案名稱，而是想直接採用檔案在 server 的名字，則可以使用 `-O <URL>` 或 `--remote-name <URL>`：

```bash
curl -O https://example.com/filename.txt
# 這樣下載下來的檔案在 local 也會叫 filename.txt
```

### 上傳檔案

使用 `-T <FILE_NAME>` 或 `--upload-file <FILE_NAME>` 來上傳檔案到 server，也可以搭配 `{}` 以及 `[]` 連續上傳多個檔案：

```bash
curl -T "img[1-1000].png" ftp://ftp.example.com/
```

上方範例是上傳 local 的 img1.png, img2.png, …, img1000.png 等 1000 個檔案到 `ftp.example.com` 這台 server，使用的是 FTP protocol。

# `wget`

```bash
wget [<OPTIONS>] <URL> [<URL2> ...]
```

`wget` 可以說是 `curl` 的閹割版，但是因為 `wget` 專門用來下載資料，所以針對下載資料提供了比 `curl` 更多的 options，此外，`wget` 也提供了比 `curl` 更 reliable 的下載服務。

### 常用 Options

- `-O <LOCAL_FILE_NAME>` 指定下載後的檔名

    請注意在 `curl` 中是使用 `-o`，但在 `wget` 是使用 `-O`。

- `-i <URL_LIST_FILE_NAME>` 匯入 URL 名單

    比如你可以將所有想下載的檔案來源條列在一個叫 `urls.txt` 的檔案中，然後使用 `wget -i ./urls.txt` 一次下載所有檔案。

- `-c` 接續下載沒載完的東西

    當所處環境的網路不穩定時，有可能東西下載到一半突然斷線，如果想要在恢復連線後接著從上次停下來的地方繼續而不要從頭下載一個新的，那就可以加上這個 option。

- `-b <LOG_FILE_NAME>` 背景下載

    需要一個 `<LOG_FILE_NAME>` 的原因是因為要將下載過程中原本要 stdout 的東西改為寫入檔案。

- 帶上使用者帳號密碼

    e.g.

    ```bash
    wget --http-user=my_user --http-password=my_password http://www.example.com/my_file
    # or
    wget --ftp-user=my_user --ftp-password=my_password ftp://ftp.example.com/my_file
    ```

- `--user-agent="<USER_AGENT>"` 偽裝為瀏覽器

    正常使用 `wget` 時下載檔案時，其 user agent 會顯示 `wget` 的版本資訊：

    ```plaintext
    66.249.79.20 - - [25/Aug/2017:09:42:44 +0800] "GET /gtwang-url-128.png HTTP/1.1" 200 5289 "-" "Wget/1.14 (linux-gnu)"
    ```

    若想要偽裝成瀏覽器，則可以這麼做：
    
    ```bash
    wget --user-agent="Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko" https://example.com/example.txt
    ```

### 可以把整個網站 copy 下來？！

使用四個 options 的組合技，就可以將指定網站整個下載下來：

```bash
wget --mirror -p --convert-links -P ./my_folder https://example.com/
```

由上面範例可見，四個 options 分別是：

- `--mirror`：下載整個網站
- `-p`：下載顯示網頁所需要的所有相關檔案
- `--convert-links`：將下載網頁中的超連結，轉換為 local directory 中的位置
- `-P <DIR>`：將下載的檔案放在 `<DIR>` 目錄下

---

關於更多 `wget` 的使用方法，可以參考[這篇網路文章](https://blog.gtwang.org/linux/linux-wget-command-download-web-pages-and-files-tutorial-examples/)

# `netstat`

#TODO 

# `whois <HOST>`

#TODO 

# `traceroute <HOST>`

#TODO 

# `route`

### 取得與自己最近的 Router 的資訊

```bash
route -n get default
```

Example output:

```plaintext
   route to: default
destination: default
       mask: default
    gateway: 192.168.50.1
  interface: en0
      flags: <UP,GATEWAY,DONE,STATIC,PRCLONING,GLOBAL>
 recvpipe  sendpipe  ssthresh  rtt,msec    rttvar  hopcount      mtu     expire
       0         0         0         0         0         0      1500         0
```

# `arp`

#TODO 
